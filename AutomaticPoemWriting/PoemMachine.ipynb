{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 一、实验目的\n",
    "1. 理解和掌握循环神经网络概念及在深度学习框架中的实现。\n",
    "2. 掌握使用深度学习框架进行文本生成任务的基本流程：如数据读取、构造网\n",
    "络、训练和预测等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、 实验要求\n",
    "1. 基于 Python 语言和任意一种深度学习框架（实验指导书中使用 Pytorch 框架\n",
    "进行介绍） ，完成数据读取、网络设计、网络构建、模型训练和模型测试等过\n",
    "程，最终实现一个可以自动写诗的程序。网络结构设计要有自己的方案，不\n",
    "能与实验指导书完全相同。\n",
    "2. 随意给出首句，如给定“湖光秋月两相和”，输出模型续写的诗句。也可以根\n",
    "据自己的兴趣，进一步实现写藏头诗（不做要求） 。要求输出的诗句尽可能地\n",
    "满足汉语语法和表达习惯。实验提供预处理后的唐诗数据集，包含 57580 首\n",
    "唐诗（在课程网站下载） ，也可以使用其他唐诗数据集。\n",
    "3. 按规定时间在课程网站提交实验报告、代码以及 PPT。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、实验原理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prepare\n",
    "'''\n",
    "    data_loader  数据加载器\n",
    "    ix2word      序号到词的映射\n",
    "    word2ix      词到序号的映射\n",
    "'''\n",
    "\n",
    "dataset = np.load('./data/tang.npz', allow_pickle=True)\n",
    "data = dataset['data']\n",
    "ix2word = dataset['ix2word'].item()\n",
    "word2ix = dataset['word2ix'].item()\n",
    "data = torch.from_numpy(data)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    data,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Poetry(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Poetry, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        embeds = self.embedding(input)\n",
    "        batch_size, seq_len = input.size()\n",
    "\n",
    "        if hidden is None:\n",
    "            h_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "            c_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "        else:\n",
    "            h_0, c_0 = hidden\n",
    "\n",
    "        output, hidden = self.lstm(embeds, (h_0, c_0))\n",
    "        output = self.fc(output)\n",
    "        output = output.reshape(batch_size * seq_len, -1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyperPara import paraList\n",
    "# hyper-parameters in paraList\n",
    "model = Poetry(\n",
    "    len(word2ix),\n",
    "    embedding_dim=paraList.embedding_dim,\n",
    "    hidden_dim=paraList.hidden_dim\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=paraList.lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_meter = 0\n",
    "\n",
    "from tqdm import tqdm\n",
    "def train(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    loss_meter = 0\n",
    "\n",
    "    loop = tqdm(data_loader, total=len(data_loader))  # 使用 tqdm 创建进度条\n",
    "    loop.set_description()\n",
    "\n",
    "    for i, data in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.long()\n",
    "        input, target = data[:, :-1], data[:, 1:]\n",
    "        output, _ = model(input)\n",
    "        loss = criterion(output, target.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_meter += loss.item()\n",
    "        loop.set_description(f'Training Epoch [{i+1}/{len(data_loader)}]')\n",
    "        loop.set_postfix(loss=loss_meter/(i+1))\n",
    "\n",
    "\n",
    "def generate(model, start_words, ix2words, word2ix, max_length=100):\n",
    "    model.train()\n",
    "    result = list(start_words)\n",
    "    start_words_len = len(start_words)\n",
    "    input = torch.Tensor([word2ix['<START>']]).view(1, 1).long()\n",
    "    hidden = None\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_length):\n",
    "            output, hidden = model(input, hidden)\n",
    "            if i < start_words_len:\n",
    "                w = result[i]\n",
    "                input = torch.Tensor([word2ix[w]]).view(1, 1).long()\n",
    "            else:\n",
    "                top_index = output.data.topk(1)[1].item()\n",
    "                w = ix2words[top_index]\n",
    "                result.append(w)\n",
    "                input = torch.Tensor([top_index]).view(1, 1).long()\n",
    "            if w == '<EOP>':\n",
    "                del result[-1]\n",
    "                break\n",
    "\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3599/3599]: 100%|██████████| 3599/3599 [19:10<00:00,  3.13it/s, loss=2.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Say:  苟利国家生死以，不知何处不相逢。\n",
      "====> Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3599/3599]: 100%|██████████| 3599/3599 [18:48<00:00,  3.19it/s, loss=2.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Say:  苟利国家生死以，不知何事不能知。我今不得不得意，不得不得不可论。我来不得不可见，不知何事不能知。我今不得不得意，不得不得不可论。我来不得不可见，不知何事不能知。我今不得不得意，不得不得不可论。我来不得\n",
      "====> Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3599/3599]: 100%|██████████| 3599/3599 [18:53<00:00,  3.18it/s, loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Say:  苟利国家生死以，不知此地无所知。我有一身不得意，不知何事不可论。我有一身不得意，不知此地无所之。我有一身不得意，不知何事不可论。我有一身不得意，不知此地无所之。我有一身不得意，不知何事不可论。\n",
      "====> Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3599/3599]: 100%|██████████| 3599/3599 [20:08<00:00,  2.98it/s, loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Say:  苟利国家生死以，不知此地无所为。我今不见不得意，一日不得不得知。一身不得不得意，一日一日不相识。一生不得一时来，一身不得无相识。一生不得一时来，一生不得无相识。一生不得一时来，一生不得无相识。若个不能\n",
      "====> Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3599/3599]: 100%|██████████| 3599/3599 [19:58<00:00,  3.00it/s, loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Say:  苟利国家生死以，不知何事不能行。我今不见君王国，我今不是无人知。君不见君王，君不见天下之。我不见我，我不知。不知我何处，我不见我不见。我今不见，不知我不可论。不知何处，不能行。不知我，不知何处？不得，\n",
      "====> Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3599/3599]: 100%|██████████| 3599/3599 [20:39<00:00,  2.90it/s, loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Say:  苟利国家生死以，不知不得不得知。君不见一，不得一为客，不得一日一日同。我今不见一日月，不见天子不得知。我今不见一日月，不见天子不得知。我今不见一日月，不如一日不得知。我今不见一日月，不如一日不得知。不\n",
      "====> Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3599/3599]: 100%|██████████| 3599/3599 [44:29<00:00,  1.35it/s, loss=1.89]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Say:  苟利国家生死以，不知何事不能回。君不见，不见有，无人知。不知何处，不见人间人不得。一生不得，一身不得，不知何处，无事无为。一生不得，一身不同。不知何事，无以有，无一事。一生不得，一身不了。一生不得，一\n",
      "====> Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3599/3599]: 100%|██████████| 3599/3599 [48:11<00:00,  1.24it/s, loss=1.86]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Say:  苟利国家生死以，不知何事。不知何处，不得一身。一身不得，一身不同。一生不得，一身不同。一身不得，一身不同。一身不得，一身不同。一身不得，一身不同。一身不得，一身不同。一生不得，一身不死。一身不得，不得\n",
      "====> Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3599/3599]: 100%|██████████| 3599/3599 [46:21<00:00,  1.29it/s, loss=1.84]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Say:  苟利国家生死以为名，不得一生。不知何处，无以为人。我不见，不得相与。我不见，不得相。不得一，不得已。不见，不得，不得，不得，不得，不得，不得，不得，不得，不得，不得，不得，不得，不得，不得，不得，不得\n",
      "====> Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3599/3599]: 100%|██████████| 3599/3599 [20:14<00:00,  2.96it/s, loss=1.82]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Say:  苟利国家生死以，不知何事不能。有时不得一日，无事无为名。有时不得意，无事无为情。有时不得意，无事不得知。不知我何为，不得见我心。我心不可见，我心不可论。我心不可见，我心不可求。我心不可见，我心不可求。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 10\n",
    "for epoch in range(EPOCH):\n",
    "    print('====> Epoch: {}'.format(epoch+1))\n",
    "    train(model, data_loader, optimizer, criterion)\n",
    "    print('====> Say: ', end=' ')\n",
    "    print(generate(model, '苟利国家生死以', ix2word, word2ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to ./saved_models/model.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "save_dir = './saved_models'\n",
    "    # 如果目录不存在，创建目录\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model_save_path = os.path.join(save_dir, 'model.pth')\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"model saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
